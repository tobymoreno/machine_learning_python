{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Image Classification using CNN inserted with Inception Modules in Pytorch - \n",
    "### Toby Moreno\n",
    "\n",
    "Traditional ANN and classic machine learning classifiers could only push the accuracy of correctly predicting MNIST images to around 97-98%.  However, with just 12 epochs and the similar hyper-parametrs, Convolutional Neueral Network (CNN) constructed with inception modules, could overcome that limit with an accuracy closely breaking the __99%__ accuracy.  Test set: Average loss: 0.0415, Accuracy: [9876/10000, 9888/10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architecture\n",
    "\n",
    "CNNs use a variation of multilayer perceptrons designed to require minimal preprocessing.[1] They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics ...\n",
    "\n",
    "CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns the filters that in traditional algorithms were hand-engineered. This independence from prior knowledge and human effort in feature design is a major advantage.- Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](http://parse.ele.tue.nl/cluster/2/CNNArchitecture.jpg)\n",
    "\n",
    "Credit Source:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Unlike traditional Neural Networks where all layers are fully-meshed, CNNs are much efficient because it uses a variation of partially-meshed multilayer perceptrons designed to require minimal preprocessing. Also the manual, time-consuming, and very subjective feature engineering of the images typically associated in traditional NN designs doesn’t apply to CNN because they are built-in the training process.  And finally, CNN’s convolved features of low-dimension matrices, the by-product output weights of the middle layers, are spatial invariant.  This is a significant because entities, texts, or objects could be tagged similar regardless of size, orientation, and spatial position in the image."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Code below is a basic reference implementation of MNIST classification using pytorch library.  Let's take care of importing all the library: Reference: # https://github.com/pytorch/examples/blob/master/mnist/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download MNIST images\n",
    "\n",
    "For the image dataset, Pytorch has a toolkit to download MNIST data for training and testing.  Inside the package torchvision, import datasets and set the download parameter to True.  It has a built-in mechanism to transform also the data and target to tensor data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/vdumoulin/conv_arithmetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](https://tobymoreno.github.io/images/cnn_mnist.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN network architectures are commonly deep (see above). But deep networks are prone to overfitting difficult to pass gradient updates through the entire network.  Additionally, just stacking large convolution operations is computationally expensive.  Researches in Google and South Carolina came up with a wider network where filters with multiple sizes operate on the same level.  For example, the below image is the “naive” inception module. It performs convolution on an input, with 3 different sizes of filters (1x1, 3x3, 5x5). Additionally, max pooling is also performed. The outputs are concatenated and sent to the next inception module.\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1409.4842v1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified CNN with inception Modules \n",
    "\n",
    "![](https://tobymoreno.github.io/images/inception_modules.001.png)\n",
    "\n",
    "Source: https://www.youtube.com/watch?v=VxhSouuSZDY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the sample data in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Split: train\n",
       "    Root Location: ./mnist_data/\n",
       "    Transforms (if any): ToTensor()\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Split: test\n",
       "    Root Location: ./mnist_data/\n",
       "    Transforms (if any): ToTensor()\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify Code Base-Copy1.ipynb         Test2.ipynb\r\n",
      "Classify Code Base.ipynb               code_analysis-Copy1.ipynb\r\n",
      "Image Classification using CNN.ipynb   code_analysis.ipynb\r\n",
      "MNIST Pytorch.ipynb                    \u001b[34mdata\u001b[m\u001b[m/\r\n",
      "Pytorch Machine Learning Primer.ipynb  file type classification.ipynb\r\n",
      "Softmax Loss.ipynb                     \u001b[34mmnist_data\u001b[m\u001b[m/\r\n",
      "Test.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "\n",
    "DataLoader is pytorch's framework to standardized and scale up the training and testing process.\n",
    "\n",
    "As long as your dataset conforms to a certain standard interface.. it could enumerate and perform automatic shuffling to fetch the next record in the dataset\n",
    "\n",
    "You shall implement the following interface:\n",
    "\n",
    "__getitem__ and __len__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic CNN Module without Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(320, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = x.view(in_size, -1)  # flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Summary for Inception (CNN)\n",
    "\n",
    "```\n",
    "Net(\n",
    "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
    "  (conv2): Conv2d(88, 20, kernel_size=(5, 5), stride=(1, 1))\n",
    "  (incept1): InceptionA(\n",
    "    (branch1x1): Conv2d(10, 16, kernel_size=(1, 1), stride=(1, 1))\n",
    "    (branch5x5_1): Conv2d(10, 16, kernel_size=(1, 1), stride=(1, 1))\n",
    "    (branch5x5_2): Conv2d(16, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "    (branch3x3dbl_1): Conv2d(10, 16, kernel_size=(1, 1), stride=(1, 1))\n",
    "    (branch3x3dbl_2): Conv2d(16, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (branch3x3dbl_3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (branch_pool): Conv2d(10, 24, kernel_size=(1, 1), stride=(1, 1))\n",
    "  )\n",
    "  (incept2): InceptionA(\n",
    "    (branch1x1): Conv2d(20, 16, kernel_size=(1, 1), stride=(1, 1))\n",
    "    (branch5x5_1): Conv2d(20, 16, kernel_size=(1, 1), stride=(1, 1))\n",
    "    (branch5x5_2): Conv2d(16, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "    (branch3x3dbl_1): Conv2d(20, 16, kernel_size=(1, 1), stride=(1, 1))\n",
    "    (branch3x3dbl_2): Conv2d(16, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (branch3x3dbl_3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (branch_pool): Conv2d(20, 24, kernel_size=(1, 1), stride=(1, 1))\n",
    "  )\n",
    "  (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (fc): Linear(in_features=1408, out_features=10, bias=True)\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "\n",
    "class InceptionA(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionA, self).__init__()\n",
    "        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "\n",
    "        self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size=5, padding=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = nn.Conv2d(16, 24, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = nn.Conv2d(24, 24, kernel_size=3, padding=1)\n",
    "\n",
    "        self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://tobymoreno.github.io/images/codemap.png)\n",
    "\n",
    "Source: https://docs.google.com/presentation/d/1MJxGye-VZVoQfN1yFJlwwgcXxhn0jWB19ZxYYg8iKxY/edit#slide=id.g2901a5d332_174_275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(88, 20, kernel_size=5)\n",
    "\n",
    "        self.incept1 = InceptionA(in_channels=10)\n",
    "        self.incept2 = InceptionA(in_channels=20)\n",
    "\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(1408, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        x = self.incept1(x)\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = self.incept2(x)\n",
    "        x = x.view(in_size, -1)  # flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run code below couple of times: 4 to 5\n",
    "```\n",
    "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.031828\n",
    "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.005347\n",
    "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.006505\n",
    "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.003654\n",
    "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.006652\n",
    "\n",
    "Test set: Average loss: 0.0413, Accuracy: 9888/10000 (98.88%)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:87: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.015831\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.004627\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.004567\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.000091\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.007195\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.030553\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.017647\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.002286\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.072995\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.010806\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.003382\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.002586\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.041820\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.003996\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.000646\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.021575\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.051365\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.025359\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.005306\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.078589\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.005944\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.011460\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.007916\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.002512\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.030585\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.002663\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.002464\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.034087\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.003607\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.089640\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.008900\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.016576\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.065113\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.050585\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.003378\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.013806\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.018109\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.005223\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.005788\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.001479\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.050487\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.012829\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.024416\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.001659\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.040967\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.024520\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.001390\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.013597\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.004465\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.083630\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.028783\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.015115\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.059596\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.038511\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.004116\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.002422\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.028397\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.143151\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.029197\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.007472\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.005875\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.031006\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.081242\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.022880\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.004873\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.038878\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.043479\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.039351\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.012015\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.024269\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.002104\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.060649\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.131799\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.035949\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.002675\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.012404\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.030560\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.019204\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.000986\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.006742\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.015281\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.023747\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.010174\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.009701\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.003825\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.011312\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.043794\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.051744\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.003851\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.010364\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.012959\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.001386\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.004563\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.004106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:115: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0426, Accuracy: 9877/10000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.013732\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.004916\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.003251\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.038598\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.024728\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.018196\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.027254\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.012725\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.026995\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.010221\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.037582\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.003763\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.005803\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.113117\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.011915\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.006110\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.046443\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.002394\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.061287\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.008085\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.000558\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.002714\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.003454\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.071705\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.006669\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.003471\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.011917\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.038722\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.010741\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.002637\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.010558\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.071002\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.000603\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.003067\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.019419\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.022002\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.013939\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.012509\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.005929\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.002888\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.007197\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.026821\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.003198\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.072948\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.001249\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.028148\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.066688\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.002548\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.007940\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.004321\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.004326\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.013278\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.024186\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.006557\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.022642\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.000654\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.017401\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.006677\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.021003\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.124586\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.002060\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.007648\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.008045\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.014283\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.016168\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.005558\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.001653\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.004448\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.074963\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.051059\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.005744\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.013998\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.011664\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.010360\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.006484\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.006183\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.024120\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.006415\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.005071\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.054094\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.004548\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.036524\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.014353\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.002508\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.059553\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.004024\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.040696\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.004292\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.029021\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.033918\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.025293\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.007176\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.000863\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.001446\n",
      "\n",
      "Test set: Average loss: 0.0407, Accuracy: 9881/10000 (98%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.011805\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.003962\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.005204\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.004572\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.010292\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.002028\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.006715\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.014384\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.019256\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.048254\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.002645\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.018793\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.007112\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.061134\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.004874\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.025207\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.008992\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.000550\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.095885\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.028159\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.143007\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.035723\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.031981\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.068803\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.001057\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.029297\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.005580\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.004358\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.032933\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.008650\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.003436\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.002016\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.003890\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.007000\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.007825\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.000533\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.004508\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.002465\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.054656\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.029244\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.093548\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.039394\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.004087\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.017805\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.001490\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.009830\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.015117\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.011687\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.038663\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.003031\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.013137\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.005479\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.002060\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.013086\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.010102\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.009144\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.002893\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.001417\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.026376\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.050769\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.023632\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.007682\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.000670\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.042881\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.001356\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.004672\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.013242\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.009907\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.057324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.005072\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.013483\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.003587\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.003143\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.005576\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.001818\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.041485\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.029820\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.008391\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.007983\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.045594\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.002832\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.018932\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.070114\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.015018\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.020849\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.025225\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.023740\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.010051\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.015681\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.016488\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.003306\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.022586\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.028260\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.012342\n",
      "\n",
      "Test set: Average loss: 0.0442, Accuracy: 9874/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.004070\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.012782\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.002505\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.039027\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.005673\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.004347\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.001118\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.006425\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.011641\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.002812\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.021882\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.022977\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.001612\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.078400\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.001727\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.023922\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.036565\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.003957\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.094436\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.010065\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.015778\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.016890\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.079197\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.001895\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.006406\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.009700\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.045449\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.007041\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.012152\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.110157\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.003057\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.014257\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.007394\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.003259\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.002762\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.001459\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.001888\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.027547\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.010657\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.034440\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.001923\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.002081\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.008317\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.024512\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.002853\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.005195\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.001656\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.030974\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.004938\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.094473\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.004063\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.000330\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.004527\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.002158\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.003049\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.004645\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.018097\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.031257\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.035283\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.020022\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.001002\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.021832\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.025675\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.032063\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.005838\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.014713\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.006627\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.001342\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.008068\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.018662\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.000611\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.023795\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.006112\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.033151\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.014057\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.012499\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.011760\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.009830\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.082728\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.029615\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.007983\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.062296\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.013845\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.008337\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.025740\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.011543\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.000222\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.033568\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.010948\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.005097\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.004667\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.003877\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.004308\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.005786\n",
      "\n",
      "Test set: Average loss: 0.0401, Accuracy: 9887/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.004774\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.006914\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.010717\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.081691\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.002173\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.017208\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.025502\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.005030\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.063460\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.032873\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.021501\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.031379\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.010279\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.053314\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.005957\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.001994\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.002770\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.048175\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.002033\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.007247\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.052846\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.023896\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.007897\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.000981\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.003546\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.012691\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.093879\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.001874\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.022876\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.001372\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.010871\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.002426\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.006859\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.103347\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.021732\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.023399\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.003400\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.047001\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.002165\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.000463\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.031003\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.002407\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.015148\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.137797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.037473\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.004423\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.003373\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.098624\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.026053\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.002063\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.039637\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.004274\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.051788\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.031249\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.008972\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.054904\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.031542\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.001057\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.036127\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.003358\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.043570\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.008284\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.021081\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.027123\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.017685\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.019542\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.120412\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.019083\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.019029\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.002621\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.014712\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.011031\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.010000\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.008922\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.047930\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.061465\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.019012\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.022136\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.006138\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.018435\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.015467\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.076691\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.059681\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.008597\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.000743\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.001466\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.035396\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.001513\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.000199\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.026918\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.054756\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.000996\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.072916\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.010616\n",
      "\n",
      "Test set: Average loss: 0.0422, Accuracy: 9875/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.034670\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.013705\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.006887\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.029474\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.006736\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.005320\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.006000\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.014308\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.024565\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.010734\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.135548\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.022404\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.000207\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.015514\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.009902\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.009086\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.003205\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.011558\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.062406\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.002858\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.016916\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.005341\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.026653\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.013920\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.000307\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.027646\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.042751\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.050407\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.038003\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.008486\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.002274\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.010075\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.001871\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.040139\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.001902\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.033237\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.014986\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.034787\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.003690\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.002605\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.009434\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.004656\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.026881\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.004750\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.003343\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.070140\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.012820\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.005493\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.038385\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.011489\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.000602\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.006832\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.007128\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.004629\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.039107\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.007568\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.050502\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.249230\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.006354\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.021617\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.018002\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.030942\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.006488\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.003448\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.000815\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.002128\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.022020\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.028083\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.003665\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.046912\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.002370\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.002314\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.003563\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.001947\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.000234\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.007719\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.000515\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.012633\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.000617\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.000425\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.076218\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.001688\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.032516\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.002850\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.036852\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.020768\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.001459\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.000683\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.012905\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.004649\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.006645\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.000928\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.007399\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.000570\n",
      "\n",
      "Test set: Average loss: 0.0460, Accuracy: 9869/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.004700\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.028411\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.007079\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.000965\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.003334\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.000676\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.000696\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.000061\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.014123\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.004652\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.001823\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.071814\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.004368\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.022190\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.006508\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.001143\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.000688\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.002468\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.015200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.009544\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.001391\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.010577\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.002078\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.000450\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.007443\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.017045\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.009549\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.019111\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.006644\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.006813\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.047441\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.016003\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.033330\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.058550\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.000079\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.028814\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.000581\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.014159\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.012409\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.002582\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.005756\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.006210\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.010329\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.000300\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.145054\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.000937\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.002539\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.049644\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.047871\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.028054\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.062238\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.005961\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.041587\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.027649\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.004256\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.004117\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.003796\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.000131\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.006361\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.004136\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.014452\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.001122\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.045555\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.001825\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.002546\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.005981\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.001783\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.025914\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.032841\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.003393\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.004811\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.024133\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.031647\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.008383\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.011339\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.007886\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.002822\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.007909\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.003667\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.000743\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.008434\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.060511\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.001232\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.008574\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.045894\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.009256\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.350796\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.003505\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.067281\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.008771\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.007696\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.001049\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.002912\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.000693\n",
      "\n",
      "Test set: Average loss: 0.0417, Accuracy: 9886/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.043214\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.004379\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.013902\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.036573\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.006004\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.004643\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.001432\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.007388\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.043164\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.002933\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.017441\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.034394\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.008815\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.001246\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.106457\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.029205\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.003836\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.003176\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.008914\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.001383\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.022878\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.010640\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.001309\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.031820\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.006379\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.017065\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.002474\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.014102\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.013205\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.003219\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.001331\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.043907\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.013392\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.041552\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.002637\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.009477\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.008238\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.029940\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.019396\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.018155\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.003230\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.022527\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.117717\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.028482\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.036464\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.008876\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.128552\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.001386\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.012111\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.001342\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.055159\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.015023\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.018518\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.007112\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.010762\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.034458\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.010091\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.006799\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.007528\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.029570\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.003030\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.010711\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.009720\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.039350\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.026046\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.007135\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.011332\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.003624\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.006780\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.003506\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.002167\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.004470\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.006367\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.007588\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.000936\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.026673\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.007918\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.028386\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.007417\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.043292\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.055789\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.001022\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.000471\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.002647\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.064953\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.013209\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.006302\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.002649\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.017653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.031828\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.005347\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.006505\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.003654\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.006652\n",
      "\n",
      "Test set: Average loss: 0.0413, Accuracy: 9888/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.011973\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.009975\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.015808\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.038299\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.006253\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.005927\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.009949\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.001249\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.009757\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.025707\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.030619\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.012213\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.002982\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.014038\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.006517\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.005463\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.002061\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.007312\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.013158\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.004977\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.064073\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.011524\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.006101\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.006981\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.000854\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.044043\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.008820\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.000960\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.004783\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.009332\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.023546\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.012177\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.002266\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.017981\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.045756\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.027428\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.000424\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.010322\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.006092\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.010369\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.002610\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.001690\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.004615\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.013719\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.009086\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.000527\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.001247\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.091528\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.117368\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.022866\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.000829\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.003139\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.131886\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.012246\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.153753\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.021915\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.003460\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.000685\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.083998\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.018179\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.002263\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.002270\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.001302\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.003032\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.040285\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.024577\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.023550\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.050704\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.003650\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.079102\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.060609\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.012708\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.003231\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.001944\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.014540\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.000069\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.000212\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.029184\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.001056\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.006342\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.018624\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.005840\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.000235\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.005832\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.020922\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.003748\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.005728\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.001131\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.000243\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.015002\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.006340\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.000483\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.000430\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.005794\n",
      "\n",
      "Test set: Average loss: 0.0453, Accuracy: 9875/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
