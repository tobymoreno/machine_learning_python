{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.rand(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3875, 0.2818, 0.8161, 0.4079, 0.5021],\n",
      "        [0.5409, 0.3984, 0.0790, 0.5562, 0.2066],\n",
      "        [0.0469, 0.0131, 0.8942, 0.7080, 0.0885]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join, splitext\n",
    "from os import walk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_labels = [\".cs\", \".js\", \".vb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = [\"var\",\"this\",\"function\",\"public\",\"namespace\",\"using\",\"System\",\"public\", \"enum\", \"Function\", \"Module\"];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.6478331745124025e-06\n",
      "1 4.447530517778867e-06\n",
      "2 4.255845253428673e-06\n",
      "3 4.072434278211784e-06\n",
      "4 3.896943539210209e-06\n",
      "5 3.72900092271829e-06\n",
      "6 3.56830330322634e-06\n",
      "7 3.414562588431623e-06\n",
      "8 3.26741223802107e-06\n",
      "9 3.1266229295497772e-06\n",
      "10 2.991889043257991e-06\n",
      "11 2.862962005653955e-06\n",
      "12 2.739599737948123e-06\n",
      "13 2.621563087692062e-06\n",
      "14 2.5085986760707127e-06\n",
      "15 2.4005366409057413e-06\n",
      "16 2.297109583317086e-06\n",
      "17 2.1981331874125037e-06\n",
      "18 2.103424161920169e-06\n",
      "19 2.0128063285458044e-06\n",
      "20 1.9260794202368994e-06\n",
      "21 1.8430970290102803e-06\n",
      "22 1.7636892923774987e-06\n",
      "23 1.6877086861451022e-06\n",
      "24 1.6150018486366674e-06\n",
      "25 1.545423408766148e-06\n",
      "26 1.4788494235533098e-06\n",
      "27 1.4151392614879742e-06\n",
      "28 1.3541741514242138e-06\n",
      "29 1.2958391952354188e-06\n",
      "30 1.2400129012028586e-06\n",
      "31 1.1865931450718013e-06\n",
      "32 1.1354832368060026e-06\n",
      "33 1.0865735081101888e-06\n",
      "34 1.0397671371666329e-06\n",
      "35 9.949807649306661e-07\n",
      "36 9.521188312835877e-07\n",
      "37 9.111065200752832e-07\n",
      "38 8.718630126171076e-07\n",
      "39 8.3430714233709e-07\n",
      "40 7.983753707894214e-07\n",
      "41 7.639847559771267e-07\n",
      "42 7.310789672584357e-07\n",
      "43 6.995905143717182e-07\n",
      "44 6.694575788122417e-07\n",
      "45 6.406248794651604e-07\n",
      "46 6.130358242207741e-07\n",
      "47 5.866302227893745e-07\n",
      "48 5.613658525980807e-07\n",
      "49 5.371868829097187e-07\n",
      "50 5.140502018800283e-07\n",
      "51 4.919160262996251e-07\n",
      "52 4.707310975367353e-07\n",
      "53 4.504559739719156e-07\n",
      "54 4.310578406679813e-07\n",
      "55 4.1249296741931474e-07\n",
      "56 3.947285031223454e-07\n",
      "57 3.777316121682771e-07\n",
      "58 3.614644447929796e-07\n",
      "59 3.458973968388214e-07\n",
      "60 3.3100404374590504e-07\n",
      "61 3.167516669449959e-07\n",
      "62 3.031122997678772e-07\n",
      "63 2.900608089324487e-07\n",
      "64 2.7756978208928523e-07\n",
      "65 2.6561805851302294e-07\n",
      "66 2.54179413354754e-07\n",
      "67 2.432341721167645e-07\n",
      "68 2.3276061357434647e-07\n",
      "69 2.227386688627481e-07\n",
      "70 2.1314832528640626e-07\n",
      "71 2.0397093065838998e-07\n",
      "72 1.9518783250670259e-07\n",
      "73 1.8678338962365717e-07\n",
      "74 1.7874096062425e-07\n",
      "75 1.7104504349131622e-07\n",
      "76 1.6368160809314223e-07\n",
      "77 1.5663356519195377e-07\n",
      "78 1.498901010160795e-07\n",
      "79 1.4343660593094838e-07\n",
      "80 1.3726086317761175e-07\n",
      "81 1.3135093125923614e-07\n",
      "82 1.25696565886965e-07\n",
      "83 1.2028454951376053e-07\n",
      "84 1.1510569728601073e-07\n",
      "85 1.1015021702960773e-07\n",
      "86 1.0540790213790193e-07\n",
      "87 1.0087061614811624e-07\n",
      "88 9.652771641932323e-08\n",
      "89 9.237244585727155e-08\n",
      "90 8.839572225364597e-08\n",
      "91 8.45901595961139e-08\n",
      "92 8.094826324823224e-08\n",
      "93 7.746373281632188e-08\n",
      "94 7.41288433313673e-08\n",
      "95 7.093821732768735e-08\n",
      "96 6.788451315596866e-08\n",
      "97 6.496214316457699e-08\n",
      "98 6.216592358562235e-08\n",
      "99 5.948966917255044e-08\n",
      "100 5.692863569459077e-08\n",
      "101 5.447835779803881e-08\n",
      "102 5.2133213072748165e-08\n",
      "103 4.9888974254015694e-08\n",
      "104 4.7741686705386054e-08\n",
      "105 4.568680643444157e-08\n",
      "106 4.372017515160763e-08\n",
      "107 4.183841706997056e-08\n",
      "108 4.0037370757864666e-08\n",
      "109 3.8314461516463873e-08\n",
      "110 3.6665248309756035e-08\n",
      "111 3.5087039397746645e-08\n",
      "112 3.357677969829953e-08\n",
      "113 3.213167414911887e-08\n",
      "114 3.074864995086627e-08\n",
      "115 2.9425382004419143e-08\n",
      "116 2.8158831023645265e-08\n",
      "117 2.6946861282025537e-08\n",
      "118 2.5787032054519704e-08\n",
      "119 2.4677057541054416e-08\n",
      "120 2.3615129421598607e-08\n",
      "121 2.2598749045781316e-08\n",
      "122 2.16260606814636e-08\n",
      "123 2.0695351651647275e-08\n",
      "124 1.980459845735638e-08\n",
      "125 1.89522791193509e-08\n",
      "126 1.8136747840311073e-08\n",
      "127 1.7356174181395476e-08\n",
      "128 1.6609250788057415e-08\n",
      "129 1.5894390239699837e-08\n",
      "130 1.521033466948034e-08\n",
      "131 1.4555746497510474e-08\n",
      "132 1.3929383225933902e-08\n",
      "133 1.3329910396457957e-08\n",
      "134 1.275633163630124e-08\n",
      "135 1.2207354966033683e-08\n",
      "136 1.1682022144810154e-08\n",
      "137 1.117927948563106e-08\n",
      "138 1.0698231668284096e-08\n",
      "139 1.0237871052814563e-08\n",
      "140 9.797275788101943e-09\n",
      "141 9.375643130589168e-09\n",
      "142 8.972196989678082e-09\n",
      "143 8.58610701716081e-09\n",
      "144 8.216659963286231e-09\n",
      "145 7.863134199741526e-09\n",
      "146 7.52475105271916e-09\n",
      "147 7.20095987026266e-09\n",
      "148 6.891075159233107e-09\n",
      "149 6.594556722676334e-09\n",
      "150 6.310802461449272e-09\n",
      "151 6.039266736831261e-09\n",
      "152 5.779393485149716e-09\n",
      "153 5.530713534048817e-09\n",
      "154 5.29281091954883e-09\n",
      "155 5.065060398960888e-09\n",
      "156 4.847115285457916e-09\n",
      "157 4.63857063310859e-09\n",
      "158 4.438987022826823e-09\n",
      "159 4.2479934899564894e-09\n",
      "160 4.06520300322181e-09\n",
      "161 3.8902966376563145e-09\n",
      "162 3.7229074309955163e-09\n",
      "163 3.562738519734998e-09\n",
      "164 3.4094589145776843e-09\n",
      "165 3.2627626693504644e-09\n",
      "166 3.1223890199870416e-09\n",
      "167 2.9880454207421846e-09\n",
      "168 2.8594750372330746e-09\n",
      "169 2.736466820813663e-09\n",
      "170 2.618735337731201e-09\n",
      "171 2.506063943837556e-09\n",
      "172 2.3982413558289708e-09\n",
      "173 2.295056545215775e-09\n",
      "174 2.1963322541958376e-09\n",
      "175 2.101851254617637e-09\n",
      "176 2.0114175927400613e-09\n",
      "177 1.924886784087521e-09\n",
      "178 1.8420697658657245e-09\n",
      "179 1.7628238637299242e-09\n",
      "180 1.686980938181889e-09\n",
      "181 1.6144029047900882e-09\n",
      "182 1.544963132010376e-09\n",
      "183 1.4784939430943887e-09\n",
      "184 1.4148868320756896e-09\n",
      "185 1.3540272178870024e-09\n",
      "186 1.2957755473650073e-09\n",
      "187 1.240035165149502e-09\n",
      "188 1.1866940119936451e-09\n",
      "189 1.1356400137079966e-09\n",
      "190 1.0867887812606361e-09\n",
      "191 1.0400352599617653e-09\n",
      "192 9.95295054997923e-10\n",
      "193 9.52482353052999e-10\n",
      "194 9.115100356461739e-10\n",
      "195 8.723053293810967e-10\n",
      "196 8.347798603663276e-10\n",
      "197 7.988699460561896e-10\n",
      "198 7.645080269916965e-10\n",
      "199 7.316192898051052e-10\n",
      "200 7.001508100059339e-10\n",
      "201 6.700400767482797e-10\n",
      "202 6.412175845136889e-10\n",
      "203 6.136371230150451e-10\n",
      "204 5.872405761844156e-10\n",
      "205 5.619817345253435e-10\n",
      "206 5.378127386862977e-10\n",
      "207 5.146784362578643e-10\n",
      "208 4.925399408838614e-10\n",
      "209 4.71354779075923e-10\n",
      "210 4.510792160951663e-10\n",
      "211 4.3167835083965134e-10\n",
      "212 4.1311207730818667e-10\n",
      "213 3.9534229115029434e-10\n",
      "214 3.783388618156582e-10\n",
      "215 3.6206485090825846e-10\n",
      "216 3.4649317156498655e-10\n",
      "217 3.315905199576846e-10\n",
      "218 3.173296794689662e-10\n",
      "219 3.036816008998926e-10\n",
      "220 2.906197926290855e-10\n",
      "221 2.781197333882072e-10\n",
      "222 2.661584887859914e-10\n",
      "223 2.5471128678131337e-10\n",
      "224 2.437574415590093e-10\n",
      "225 2.332731159192672e-10\n",
      "226 2.2324010039328817e-10\n",
      "227 2.1363977727260189e-10\n",
      "228 2.0445162483348425e-10\n",
      "229 1.9565795838000568e-10\n",
      "230 1.872436983945142e-10\n",
      "231 1.7919149911032449e-10\n",
      "232 1.714849980599214e-10\n",
      "233 1.641101341128176e-10\n",
      "234 1.5705201401283456e-10\n",
      "235 1.5029787881013528e-10\n",
      "236 1.4383384630564783e-10\n",
      "237 1.376496896488521e-10\n",
      "238 1.3172995051131198e-10\n",
      "239 1.2606462253850964e-10\n",
      "240 1.2064354791290035e-10\n",
      "241 1.1545473701721322e-10\n",
      "242 1.1048953708999473e-10\n",
      "243 1.0573889973742046e-10\n",
      "244 1.0119132424595535e-10\n",
      "245 9.683995120678975e-11\n",
      "246 9.267619172756978e-11\n",
      "247 8.869057820256551e-11\n",
      "248 8.487695480154319e-11\n",
      "249 8.12275454716523e-11\n",
      "250 7.773464936708661e-11\n",
      "251 7.439174626508205e-11\n",
      "252 7.119267430912825e-11\n",
      "253 6.813146636336565e-11\n",
      "254 6.520162898663873e-11\n",
      "255 6.239798521600314e-11\n",
      "256 5.971489455545588e-11\n",
      "257 5.714703568685525e-11\n",
      "258 5.4689830526112544e-11\n",
      "259 5.233827082264316e-11\n",
      "260 5.008770148083707e-11\n",
      "261 4.7934220325546885e-11\n",
      "262 4.587293162592927e-11\n",
      "263 4.390056658384274e-11\n",
      "264 4.2012843621327526e-11\n",
      "265 4.020623943572042e-11\n",
      "266 3.847747608108134e-11\n",
      "267 3.682313554084122e-11\n",
      "268 3.523975510473483e-11\n",
      "269 3.372462058813384e-11\n",
      "270 3.2274541207608355e-11\n",
      "271 3.088694793218151e-11\n",
      "272 2.955887144580546e-11\n",
      "273 2.8288069738163535e-11\n",
      "274 2.7071793967971164e-11\n",
      "275 2.5907783294845948e-11\n",
      "276 2.4793886012992212e-11\n",
      "277 2.3727794515704206e-11\n",
      "278 2.270762225033438e-11\n",
      "279 2.1731452096832852e-11\n",
      "280 2.0797039250394973e-11\n",
      "281 1.9902919507550675e-11\n",
      "282 1.9047263217025828e-11\n",
      "283 1.8228313957806206e-11\n",
      "284 1.7444641232070882e-11\n",
      "285 1.6694676282019508e-11\n",
      "286 1.59769248092454e-11\n",
      "287 1.529001868983951e-11\n",
      "288 1.4632656334448083e-11\n",
      "289 1.4003572795872325e-11\n",
      "290 1.3401496377399435e-11\n",
      "291 1.2825380990779959e-11\n",
      "292 1.2274008241124302e-11\n",
      "293 1.1746427853977672e-11\n",
      "294 1.1241502730467701e-11\n",
      "295 1.075820372823485e-11\n",
      "296 1.0295717905479221e-11\n",
      "297 9.853125294581351e-12\n",
      "298 9.429530050889315e-12\n",
      "299 9.024159150659465e-12\n",
      "300 8.636192532769703e-12\n",
      "301 8.264917727461556e-12\n",
      "302 7.909624132101448e-12\n",
      "303 7.56962552056792e-12\n",
      "304 7.244225202832155e-12\n",
      "305 6.932816818347698e-12\n",
      "306 6.63477801076307e-12\n",
      "307 6.349579817321364e-12\n",
      "308 6.076620934124106e-12\n",
      "309 5.815425021695779e-12\n",
      "310 5.56542763121132e-12\n",
      "311 5.326179728916189e-12\n",
      "312 5.097237909525672e-12\n",
      "313 4.878119139382821e-12\n",
      "314 4.668451297184181e-12\n",
      "315 4.467760707392817e-12\n",
      "316 4.275708626795965e-12\n",
      "317 4.091939406978866e-12\n",
      "318 3.916041789664605e-12\n",
      "319 3.747714364214661e-12\n",
      "320 3.586615303070316e-12\n",
      "321 3.4324593685187125e-12\n",
      "322 3.2849185443936918e-12\n",
      "323 3.1437134998173956e-12\n",
      "324 3.0085900544583595e-12\n",
      "325 2.8792635803911598e-12\n",
      "326 2.7555124195877423e-12\n",
      "327 2.637078988688905e-12\n",
      "328 2.5237270451790073e-12\n",
      "329 2.4152621188402885e-12\n",
      "330 2.311443131918652e-12\n",
      "331 2.212094061703699e-12\n",
      "332 2.117025980263303e-12\n",
      "333 2.0260276099650705e-12\n",
      "334 1.93895004779399e-12\n",
      "335 1.8556120589827295e-12\n",
      "336 1.7758518723899606e-12\n",
      "337 1.699527630102902e-12\n",
      "338 1.6264861333618497e-12\n",
      "339 1.556581629422399e-12\n",
      "340 1.489691053751622e-12\n",
      "341 1.4256693943954426e-12\n",
      "342 1.364396350677996e-12\n",
      "343 1.3057530619247182e-12\n",
      "344 1.2496353207990862e-12\n",
      "345 1.195933339036515e-12\n",
      "346 1.1445319406633795e-12\n",
      "347 1.0953428361886656e-12\n",
      "348 1.0482663470240766e-12\n",
      "349 1.0032170568462172e-12\n",
      "350 9.60102999612241e-13\n",
      "351 9.18839092343723e-13\n",
      "352 8.793527891627159e-13\n",
      "353 8.415635524768127e-13\n",
      "354 8.053976914399016e-13\n",
      "355 7.707836557949282e-13\n",
      "356 7.376604530610766e-13\n",
      "357 7.059607512118198e-13\n",
      "358 6.756196319233144e-13\n",
      "359 6.465870258515578e-13\n",
      "360 6.187988877194122e-13\n",
      "361 5.922060903461509e-13\n",
      "362 5.667585003379534e-13\n",
      "363 5.42402775280337e-13\n",
      "364 5.190947823175834e-13\n",
      "365 4.967857748030603e-13\n",
      "366 4.754386956083817e-13\n",
      "367 4.550085064556871e-13\n",
      "368 4.3545741781242467e-13\n",
      "369 4.167448395179035e-13\n",
      "370 3.9883628132198907e-13\n",
      "371 3.816981672939892e-13\n",
      "372 3.6529560155939333e-13\n",
      "373 3.495975720625035e-13\n",
      "374 3.3457665744269535e-13\n",
      "375 3.201991813642624e-13\n",
      "376 3.064403219104692e-13\n",
      "377 2.932725507297479e-13\n",
      "378 2.8067005157293885e-13\n",
      "379 2.6861129630647756e-13\n",
      "380 2.570699568961132e-13\n",
      "381 2.460241477045249e-13\n",
      "382 2.3545234026578365e-13\n",
      "383 2.2533474522838824e-13\n",
      "384 2.1565258328620804e-13\n",
      "385 2.0638580164398026e-13\n",
      "386 1.975212289583999e-13\n",
      "387 1.890339461461834e-13\n",
      "388 1.8091101163834045e-13\n",
      "389 1.731382616076886e-13\n",
      "390 1.656989082831818e-13\n",
      "391 1.5857958523068467e-13\n",
      "392 1.5176677634318347e-13\n",
      "393 1.4524621399764488e-13\n",
      "394 1.390053647524235e-13\n",
      "395 1.3303300940999842e-13\n",
      "396 1.2731718448104148e-13\n",
      "397 1.218467856456074e-13\n",
      "398 1.1661227865702228e-13\n",
      "399 1.1160204884581449e-13\n",
      "400 1.0680703571515446e-13\n",
      "401 1.0221816937158801e-13\n",
      "402 9.782625598205766e-14\n",
      "403 9.362349749771006e-14\n",
      "404 8.960192530801067e-14\n",
      "405 8.575201061364517e-14\n",
      "406 8.206807330312293e-14\n",
      "407 7.854202926501495e-14\n",
      "408 7.516799379163545e-14\n",
      "409 7.193842336701432e-14\n",
      "410 6.884817477386117e-14\n",
      "411 6.589027319589287e-14\n",
      "412 6.305947203325025e-14\n",
      "413 6.035048413069078e-14\n",
      "414 5.775763529280995e-14\n",
      "415 5.527649038166593e-14\n",
      "416 5.290194744315278e-14\n",
      "417 5.062917932458703e-14\n",
      "418 4.8454486081888594e-14\n",
      "419 4.6372835595906585e-14\n",
      "420 4.438080915190941e-14\n",
      "421 4.247435896155671e-14\n",
      "422 4.064980052925078e-14\n",
      "423 3.890349512549661e-14\n",
      "424 3.7232186170856165e-14\n",
      "425 3.563288799058997e-14\n",
      "426 3.410206194934556e-14\n",
      "427 3.2637340818815e-14\n",
      "428 3.1235294632470855e-14\n",
      "429 2.989353126221613e-14\n",
      "430 2.8609432530338955e-14\n",
      "431 2.7380579263828663e-14\n",
      "432 2.62044762770249e-14\n",
      "433 2.5078856980377154e-14\n",
      "434 2.400189125306806e-14\n",
      "435 2.2970801251215942e-14\n",
      "436 2.1984047113802903e-14\n",
      "437 2.1039805666027217e-14\n",
      "438 2.013605663945436e-14\n",
      "439 1.9271217054068247e-14\n",
      "440 1.8443419573855974e-14\n",
      "441 1.7651249268050602e-14\n",
      "442 1.6893046962841633e-14\n",
      "443 1.616742531757577e-14\n",
      "444 1.547299546844901e-14\n",
      "445 1.4808492545431622e-14\n",
      "446 1.4172494044366776e-14\n",
      "447 1.3563746185017612e-14\n",
      "448 1.2981140914706401e-14\n",
      "449 1.2423610333513335e-14\n",
      "450 1.1889972475351535e-14\n",
      "451 1.1379362118279178e-14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452 1.0890595180309977e-14\n",
      "453 1.0422854173143346e-14\n",
      "454 9.975164450096885e-15\n",
      "455 9.546759591952253e-15\n",
      "456 9.136731149874479e-15\n",
      "457 8.744343192166619e-15\n",
      "458 8.368790717221496e-15\n",
      "459 8.009405856847016e-15\n",
      "460 7.665430483312545e-15\n",
      "461 7.336204876375139e-15\n",
      "462 7.021137165452892e-15\n",
      "463 6.719607446481848e-15\n",
      "464 6.4310191498908385e-15\n",
      "465 6.154835369868072e-15\n",
      "466 5.890493495907301e-15\n",
      "467 5.637514997180975e-15\n",
      "468 5.395431706939745e-15\n",
      "469 5.163717529790208e-15\n",
      "470 4.94196751041267e-15\n",
      "471 4.72974095454086e-15\n",
      "472 4.526614629437077e-15\n",
      "473 4.3322356842270516e-15\n",
      "474 4.146209246928567e-15\n",
      "475 3.968163086051054e-15\n",
      "476 3.7977535934024265e-15\n",
      "477 3.634654785965996e-15\n",
      "478 3.47857616961652e-15\n",
      "479 3.3292021628186944e-15\n",
      "480 3.1862476370529708e-15\n",
      "481 3.049451455272467e-15\n",
      "482 2.9184975563205556e-15\n",
      "483 2.7931707448790844e-15\n",
      "484 2.6732335940596218e-15\n",
      "485 2.558452577453594e-15\n",
      "486 2.4485848476451823e-15\n",
      "487 2.343441687948842e-15\n",
      "488 2.2428292802918252e-15\n",
      "489 2.14652106260351e-15\n",
      "490 2.0543497841834516e-15\n",
      "491 1.966150460212899e-15\n",
      "492 1.8817205550886325e-15\n",
      "493 1.8009242356460504e-15\n",
      "494 1.7235909962492142e-15\n",
      "495 1.6495833870696885e-15\n",
      "496 1.5787555263808944e-15\n",
      "497 1.5109792694457628e-15\n",
      "498 1.4461003001624e-15\n",
      "499 1.3840061453729118e-15\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    # do a forward-pass by calculating predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0.001)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    \n",
    "    #calculate loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "    \n",
    "    # Backprop to compute the gradients of w1 and w2 with respect to loss\n",
    "    \n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    #Update Weights\n",
    "\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.51711514],\n",
       "       [ 0.16688039],\n",
       "       [-0.44993735],\n",
       "       [ 0.21578307]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], np.int32)\n",
    "y = np.array([[0], [1], [1], [0]], np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    index = (randint(0, 3))\n",
    "    #print(x[index], y[index])\n",
    "    x = np.append(x, [x[index]], 0)\n",
    "    y = np.append(y, [y[index]], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1004, 2), (1004, 1))"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = x.shape[0], x.shape[1], 8, y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1004, 2)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1004, 2, 8, 1)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, D_in, H, D_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 11436.743568213145\n",
      "10000 0.013129347670475171\n",
      "20000 0.013120795161710604\n",
      "30000 0.013119824653655979\n",
      "40000 0.01311885422069104\n",
      "50000 0.01311788387207832\n",
      "60000 0.013116913596339104\n",
      "70000 0.01311594340405672\n",
      "80000 0.013114973284642749\n",
      "90000 0.013114003248685528\n"
     ]
    }
   ],
   "source": [
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(100000):\n",
    "    # do a forward-pass by calculating predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0.001)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    \n",
    "    #calculate loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 10000 == 0:\n",
    "        print(t, loss)\n",
    "        \n",
    "    if(loss < 0.005):\n",
    "        break\n",
    "    \n",
    "    # Backprop to compute the gradients of w1 and w2 with respect to loss\n",
    "    \n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    #Update Weights\n",
    "\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99999751])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx = np.array([1, 0], np.int32)\n",
    "h = nx.dot(w1)\n",
    "h_relu = np.maximum(h, 0.001)\n",
    "y_pred = h_relu.dot(w2)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got Tensor)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-262-616ff50c2468>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected np.ndarray (got Tensor)"
     ]
    }
   ],
   "source": [
    "x=torch.from_numpy(x).float()\n",
    "y=torch.from_numpy(y).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        ...,\n",
       "        [1., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 255.3799285888672\n",
      "50 0.09982776641845703\n",
      "100 0.0018633328145369887\n",
      "150 4.7891412577882875e-06\n",
      "200 5.258366542193471e-08\n",
      "250 9.426928682110258e-11\n",
      "300 3.992028929644675e-12\n",
      "350 1.861288900784075e-12\n",
      "400 4.674954867667225e-12\n",
      "450 4.665295927352986e-12\n"
     ]
    }
   ],
   "source": [
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 0.05\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 50 == 0:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.3146, -0.0017],\n",
       "         [-0.9313,  0.6113],\n",
       "         [-0.7127,  0.1330],\n",
       "         [-0.5999, -0.2080],\n",
       "         [-0.3640, -0.6911],\n",
       "         [-0.9767,  0.6422],\n",
       "         [-0.6464,  0.7695],\n",
       "         [ 1.0095, -1.4238]], requires_grad=True), Parameter containing:\n",
       " tensor([-0.0840, -0.1136, -0.1698,  0.1986, -0.3879, -0.1708, -0.2216, -0.2337],\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([[-0.2523,  0.5793, -0.1111, -0.2907,  0.2884,  0.7567,  0.5424,  1.2895]],\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([0.0577], requires_grad=True)]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx = torch.from_numpy(np.array([0, 1], np.int32)).float()\n",
    "y_pred = model(nx)\n",
    "round(y_pred.item(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        ...,\n",
       "        [1., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "at = torch.arange(0,len(y_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.cs', '.js', '.vb']"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "pY = torch.zeros(len(at), at.max()+1).scatter_(1, at.unsqueeze(1), 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do cross entropy exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0.])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pY[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.        , -0.        ,  0.35667494]),\n",
       " array([-0.        , -0.        ,  2.30258509]))"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# do one hot encoding\n",
    "# 0 : 1 0 0\n",
    "# 1 : 0 1 0\n",
    "# 2 : 0 0 1\n",
    "\n",
    "Y = np.array([0, 0, 1])\n",
    "\n",
    "Y_pred1 = np.array([0.2, 0.2, 0.7])\n",
    "Y_pred2 = np.array([0.1, 0.3, 0.10])\n",
    "\n",
    "(-Y * np.log(Y_pred1)), (-Y * np.log(Y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's do it in pytorch\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Loss1 =  tensor(0.4170) \n",
      "PyTorch Loss2= tensor(1.8406)\n"
     ]
    }
   ],
   "source": [
    "Y = Variable(torch.LongTensor([0]), requires_grad=False)\n",
    "\n",
    "# input is of size nBatch x nClasses = 1 x 4\n",
    "# Y_pred are logits (not softmax)\n",
    "Y_pred1 = Variable(torch.Tensor([[2.0, 1.0, 0.1]]))\n",
    "Y_pred2 = Variable(torch.Tensor([[0.5, 2.0, 0.3]]))\n",
    "\n",
    "l1 = loss(Y_pred1, Y)\n",
    "l2 = loss(Y_pred2, Y)\n",
    "\n",
    "print(\"PyTorch Loss1 = \", l1.data, \"\\nPyTorch Loss2=\", l2.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss1 =  tensor(0.4966) \n",
      "Batch Loss2= tensor(1.2389)\n"
     ]
    }
   ],
   "source": [
    "# target is of size nBatch\n",
    "# each element in target has to have 0 <= value < nClasses (0-2)\n",
    "# Input is class, not one-hot\n",
    "Y = Variable(torch.LongTensor([2, 0, 1]), requires_grad=False)\n",
    "\n",
    "# input is of size nBatch x nClasses = 2 x 4\n",
    "# Y_pred are logits (not softmax)\n",
    "Y_pred1 = Variable(torch.Tensor([[0.1, 0.2, 0.9],\n",
    "                                 [1.1, 0.1, 0.2],\n",
    "                                 [0.2, 2.1, 0.1]]))\n",
    "\n",
    "\n",
    "Y_pred2 = Variable(torch.Tensor([[0.8, 0.2, 0.3],\n",
    "                                 [0.2, 0.3, 0.5],\n",
    "                                 [0.2, 0.2, 0.5]]))\n",
    "\n",
    "l1 = loss(Y_pred1, Y)\n",
    "l2 = loss(Y_pred2, Y)\n",
    "\n",
    "print(\"Batch Loss1 = \", l1.data, \"\\nBatch Loss2=\", l2.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
